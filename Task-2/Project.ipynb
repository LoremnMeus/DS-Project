{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e439ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import Normalizer,StandardScaler,MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest,chi2,VarianceThreshold\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import metrics\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import DenseGraphConv\n",
    "import pickle\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader\n",
    "path = \"Project for ML/MoleculeEvaluationData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a4d7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from(path):\n",
    "    f= open(path,'rb')\n",
    "    #data = np.unpackbits(f)\n",
    "    d = pickle.load(f)\n",
    "    data = d[\"packed_fp\"]\n",
    "    label = d[\"values\"]\n",
    "    f.close()\n",
    "    return label.squeeze().numpy(),np.unpackbits(data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d2fd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label,train_data = load_from(path + \"train.pkl\")\n",
    "test_label,test_data = load_from(path + \"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "159df86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain(train_data,train_label,Num = 100000):\n",
    "    train_data2 = np.zeros([Num,2048 * 2])\n",
    "    train_label2 = np.zeros(Num)\n",
    "    for i in range(Num):\n",
    "        rnd1,rnd2 = np.random.randint(0,train_data.shape[0]),np.random.randint(0,train_data.shape[0])\n",
    "        train_data2[i] = np.concatenate([train_data[rnd1],train_data[rnd2]])\n",
    "            #        F.cosine_similarity(torch.tensor(train_data[rnd1].astype(float)),torch.tensor(train_data[rnd2].astype(float)),dim = 0).numpy()])\n",
    "        train_label2[i] = train_label[rnd1] + train_label[rnd2]\n",
    "    return train_data2,train_label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33689d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_NUM = 5\n",
    "def cos_matrix(data):\n",
    "    ret = np.zeros([TOTAL_NUM,TOTAL_NUM])\n",
    "    for i in range(TOTAL_NUM):\n",
    "        for j in range(TOTAL_NUM):\n",
    "            ret[i][j] = F.cosine_similarity(torch.tensor(data[i].astype(float)),torch.tensor(data[j].astype(float)),dim = 0).numpy()\n",
    "    return ret\n",
    "def retrain2(train_data,train_label,Num = 1000):\n",
    "    train_data2 = np.zeros([Num,TOTAL_NUM,2048])\n",
    "    train_label2 = np.zeros([Num,TOTAL_NUM])\n",
    "    ajmtx = np.zeros([Num,TOTAL_NUM,TOTAL_NUM])\n",
    "    for i in range(Num):\n",
    "        for j in range(TOTAL_NUM):\n",
    "            rnd1 = np.random.randint(0,train_data.shape[0])\n",
    "            train_data2[i][j] = train_data[rnd1]\n",
    "            train_label2[i][j] = train_label[rnd1]\n",
    "        ajmtx[i] = cos_matrix(train_data2[i])\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    return train_data2,train_label2,ajmtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1ca2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_NUM = 10\n",
    "def retrain(train_data,train_label,Num = 10000):\n",
    "    train_data2 = np.zeros([Num,TOTAL_NUM * 2048])\n",
    "    train_label2 = np.zeros(Num)\n",
    "    for i in range(Num):\n",
    "        x = 0\n",
    "        for j in range(TOTAL_NUM):\n",
    "            rnd1 = np.random.randint(0,train_data.shape[0])\n",
    "            if j == 0:\n",
    "                x = train_data[rnd1]\n",
    "            else:\n",
    "                x = np.concatenate([x,train_data[rnd1]])\n",
    "            train_label2[i] = train_label2[i] + train_label[rnd1]\n",
    "        train_data2[i] = x\n",
    "    return train_data2,train_label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f1b26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sel = PCA(n_components=128, copy=True, whiten=False)\n",
    "#scaler = StandardScaler()\n",
    "tdata = Sel.fit_transform(train_data)\n",
    "sdata = Sel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd3fe3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlabel = train_label\n",
    "slabel = test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53256843",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = train_data\n",
    "sdata = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "237b80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata,tlabel = retrain(train_data,train_label,4 * 10000)\n",
    "sdata,slabel = retrain(test_data,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb7284fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "tdata,tlabel,tajmtx = retrain2(train_data,train_label,4 * 5000)\n",
    "sdata,slabel,sajmtx = retrain2(test_data,test_label,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29612d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "print(tajmtx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328056a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 313.98475611\n",
      "Iteration 2, loss = 304.22807803\n",
      "Iteration 3, loss = 295.21666312\n",
      "Iteration 4, loss = 286.45363101\n",
      "Iteration 5, loss = 277.90643887\n",
      "Iteration 6, loss = 269.58321715\n",
      "Iteration 7, loss = 261.48073621\n",
      "Iteration 8, loss = 253.58819334\n",
      "Iteration 9, loss = 245.90245460\n",
      "Iteration 10, loss = 238.43187832\n",
      "Iteration 11, loss = 231.15528901\n",
      "Iteration 12, loss = 224.07983506\n",
      "Iteration 13, loss = 217.20015712\n",
      "Iteration 14, loss = 210.51481469\n",
      "Iteration 15, loss = 204.00804276\n",
      "Iteration 16, loss = 197.69385580\n",
      "Iteration 17, loss = 191.55384933\n",
      "Iteration 18, loss = 185.60194419\n",
      "Iteration 19, loss = 179.80699877\n",
      "Iteration 20, loss = 174.19036613\n",
      "Iteration 21, loss = 168.74695055\n",
      "Iteration 22, loss = 163.45581663\n",
      "Iteration 23, loss = 158.33535115\n",
      "Iteration 24, loss = 153.36538658\n",
      "Iteration 25, loss = 148.54585384\n",
      "Iteration 26, loss = 143.88890257\n",
      "Iteration 27, loss = 139.36997390\n",
      "Iteration 28, loss = 135.00033995\n",
      "Iteration 29, loss = 130.77299819\n",
      "Iteration 30, loss = 126.68086652\n",
      "Iteration 31, loss = 122.72522111\n",
      "Iteration 32, loss = 118.90949496\n",
      "Iteration 33, loss = 115.21545651\n",
      "Iteration 34, loss = 111.65131442\n",
      "Iteration 35, loss = 108.21577477\n",
      "Iteration 36, loss = 104.90011548\n",
      "Iteration 37, loss = 101.70087068\n",
      "Iteration 38, loss = 98.62184991\n",
      "Iteration 39, loss = 95.65591994\n",
      "Iteration 40, loss = 92.80502051\n",
      "Iteration 41, loss = 90.05726842\n",
      "Iteration 42, loss = 87.41743926\n",
      "Iteration 43, loss = 84.88100866\n",
      "Iteration 44, loss = 82.44852604\n",
      "Iteration 45, loss = 80.10590670\n",
      "Iteration 46, loss = 77.87129819\n",
      "Iteration 47, loss = 75.72209955\n",
      "Iteration 48, loss = 73.66642495\n",
      "Iteration 49, loss = 71.69952092\n",
      "Iteration 50, loss = 69.81594722\n",
      "Iteration 51, loss = 68.02171743\n",
      "Iteration 52, loss = 66.30610665\n",
      "Iteration 53, loss = 64.66676928\n",
      "Iteration 54, loss = 63.10745811\n",
      "Iteration 55, loss = 61.62074671\n",
      "Iteration 56, loss = 60.20917007\n",
      "Iteration 57, loss = 58.86751437\n",
      "Iteration 58, loss = 57.58943255\n",
      "Iteration 59, loss = 56.37816947\n",
      "Iteration 60, loss = 55.23132531\n",
      "Iteration 61, loss = 54.14535637\n",
      "Iteration 62, loss = 53.11889406\n",
      "Iteration 63, loss = 52.14837204\n",
      "Iteration 64, loss = 51.23389501\n"
     ]
    }
   ],
   "source": [
    "model = MLPRegressor(hidden_layer_sizes= (20,5), activation='relu',solver='adam',max_iter=500,\n",
    "                    learning_rate_init=0.01,learning_rate='adaptive',early_stopping=False,epsilon = 1e-5,beta_1 = 0.9,verbose = True,\n",
    "                     #tol = 1e-4,\n",
    "                    )\n",
    "model.fit(tdata,tlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98eba7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = model.predict(sdata)\n",
    "zt = model.predict(tdata)\n",
    "rate = (np.mean(abs(zt-tlabel)))\n",
    "print('偏移率:',rate)\n",
    "rate = (np.mean(abs(z-slabel)))\n",
    "print('误差率:',rate)\n",
    "rate = (np.sum(z-slabel))\n",
    "print('全误差:',rate)\n",
    "rate = (np.sum(slabel))\n",
    "print('总和:',rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5f316a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata,slabel = retrain(test_data,test_label,50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62af1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1,16,5,1,2),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool1d(2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(16,32,5,1,2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(1024, 1),\n",
    "            #nn.Dropout(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "def save_model(net, path):\n",
    "    torch.save(net, path)\n",
    "\n",
    "def load_model(path):\n",
    "    net = torch.load(path)\n",
    "    return net\n",
    "def train_model(net, data, label, lr, batch_size, epoch,early_stop = 10):\n",
    "    # net = net.cuda()\n",
    "    data = torch.Tensor(data)\n",
    "    data = data.unsqueeze(1)\n",
    "    label = torch.Tensor(label).float()\n",
    "    # data =data.cuda()\n",
    "    # label=label.cuda()\n",
    "    \n",
    "    train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.1, random_state=0)\n",
    "    \n",
    "    LR = lr\n",
    "    BATCH_SIZE = batch_size\n",
    "    EPOCH = epoch\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "    train_dataset = Data.TensorDataset(train_data, train_label)\n",
    "    train_loader = Data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    test_dataset = Data.TensorDataset(test_data, test_label)\n",
    "    test_loader = Data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, LR, epochs=EPOCH, steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    best = 1000\n",
    "    be = 0\n",
    "    for epoch in range(EPOCH):\n",
    "        for step, (batch_data, batch_label) in enumerate(train_loader):\n",
    "            if step % 1000 == 0:\n",
    "                print('Epoch:', epoch + 1, '/', EPOCH, 'Step:', step)\n",
    "            prediction = net(batch_data).squeeze()\n",
    "            loss = F.l1_loss(prediction,batch_label)#F.cross_entropy(prediction, batch_label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        accuracy = []\n",
    "        for stp, (test_x, test_y) in enumerate(test_loader):\n",
    "            test_output = net(test_x).squeeze()\n",
    "            accuracy.append(np.mean(abs(test_output-test_y).detach().numpy()))\n",
    "        succ = np.mean(accuracy)\n",
    "        if best > succ:\n",
    "            best = succ\n",
    "            save_model(net, \"tmp_cnn.pkl\")\n",
    "            be = epoch\n",
    "        if epoch - be >= early_stop:\n",
    "            net = load_model(\"tmp_cnn.pkl\")\n",
    "            print('Early Stop in Epoch', epoch + 1, '| train loss:%.4f' % loss, '| test accuracy:%.4f' % succ)\n",
    "            accuracy = []\n",
    "            for stp, (test_x, test_y) in enumerate(test_loader):\n",
    "                test_output = net(test_x).squeeze()\n",
    "                accuracy.append(np.mean(abs(test_output-test_y).detach().numpy()))\n",
    "            succ = np.mean(accuracy)\n",
    "            print('Best Epoch', be + 1, '| test accuracy:%.4f' % succ)\n",
    "            return net\n",
    "        print('Epoch', epoch + 1, '| train loss:%.4f' % loss, '| test accuracy:%.4f' % succ)\n",
    "    return net\n",
    "\n",
    "def test_model(net,data,label):\n",
    "    test_data = torch.Tensor(data).unsqueeze(1)\n",
    "    test_label = torch.Tensor(label).float()\n",
    "    \n",
    "    test_dataset = Data.TensorDataset(test_data,test_label)\n",
    "    test_loader = Data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    ret = []\n",
    "    ret2 = []\n",
    "    ret3 = []\n",
    "    for stp, (test_x,test_y) in enumerate(test_loader):\n",
    "        pred_y = net(test_x).squeeze()\n",
    "        ret.append(np.mean(np.mean(abs(pred_y-test_y).detach().numpy())))\n",
    "        ret2.append(np.sum(np.sum((pred_y-test_y).detach().numpy())))\n",
    "        ret3.append(np.sum(np.sum((test_y).detach().numpy())))\n",
    "    #print(test_y)\n",
    "    #print(pred_y)\n",
    "    \n",
    "    print(\"误差：\",np.mean(ret))\n",
    "    print(\"全误差：\",np.sum(ret2))\n",
    "    print(\"总值：\",np.sum(ret3))\n",
    "    return np.mean(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c9ef9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 30 Step: 0\n",
      "Epoch: 1 / 30 Step: 1000\n",
      "Epoch: 1 / 30 Step: 2000\n",
      "Epoch 1 | train loss:0.4790 | test accuracy:0.4533\n",
      "Epoch: 2 / 30 Step: 0\n",
      "Epoch: 2 / 30 Step: 1000\n",
      "Epoch: 2 / 30 Step: 2000\n",
      "Epoch 2 | train loss:0.4469 | test accuracy:0.4502\n",
      "Epoch: 3 / 30 Step: 0\n",
      "Epoch: 3 / 30 Step: 1000\n",
      "Epoch: 3 / 30 Step: 2000\n",
      "Epoch 3 | train loss:0.6647 | test accuracy:0.4474\n",
      "Epoch: 4 / 30 Step: 0\n",
      "Epoch: 4 / 30 Step: 1000\n",
      "Epoch: 4 / 30 Step: 2000\n",
      "Epoch 4 | train loss:0.5196 | test accuracy:0.4457\n",
      "Epoch: 5 / 30 Step: 0\n",
      "Epoch: 5 / 30 Step: 1000\n",
      "Epoch: 5 / 30 Step: 2000\n",
      "Epoch 5 | train loss:0.6862 | test accuracy:0.4475\n",
      "Epoch: 6 / 30 Step: 0\n",
      "Epoch: 6 / 30 Step: 1000\n",
      "Epoch: 6 / 30 Step: 2000\n",
      "Epoch 6 | train loss:0.4250 | test accuracy:0.4457\n",
      "Epoch: 7 / 30 Step: 0\n",
      "Epoch: 7 / 30 Step: 1000\n",
      "Epoch: 7 / 30 Step: 2000\n",
      "Epoch 7 | train loss:0.3711 | test accuracy:0.4464\n",
      "Epoch: 8 / 30 Step: 0\n",
      "Epoch: 8 / 30 Step: 1000\n",
      "Epoch: 8 / 30 Step: 2000\n",
      "Epoch 8 | train loss:0.3299 | test accuracy:0.4470\n",
      "Epoch: 9 / 30 Step: 0\n",
      "Epoch: 9 / 30 Step: 1000\n",
      "Epoch: 9 / 30 Step: 2000\n",
      "Epoch 9 | train loss:0.4470 | test accuracy:0.4460\n",
      "Epoch: 10 / 30 Step: 0\n",
      "Epoch: 10 / 30 Step: 1000\n",
      "Epoch: 10 / 30 Step: 2000\n",
      "Epoch 10 | train loss:0.5615 | test accuracy:0.4477\n",
      "Epoch: 11 / 30 Step: 0\n",
      "Epoch: 11 / 30 Step: 1000\n",
      "Epoch: 11 / 30 Step: 2000\n",
      "Epoch 11 | train loss:0.4733 | test accuracy:0.4471\n",
      "Epoch: 12 / 30 Step: 0\n",
      "Epoch: 12 / 30 Step: 1000\n",
      "Epoch: 12 / 30 Step: 2000\n",
      "Epoch 12 | train loss:0.4245 | test accuracy:0.4450\n",
      "Epoch: 13 / 30 Step: 0\n",
      "Epoch: 13 / 30 Step: 1000\n",
      "Epoch: 13 / 30 Step: 2000\n",
      "Epoch 13 | train loss:0.4314 | test accuracy:0.4454\n",
      "Epoch: 14 / 30 Step: 0\n",
      "Epoch: 14 / 30 Step: 1000\n",
      "Epoch: 14 / 30 Step: 2000\n",
      "Epoch 14 | train loss:0.6512 | test accuracy:0.4455\n",
      "Epoch: 15 / 30 Step: 0\n",
      "Epoch: 15 / 30 Step: 1000\n",
      "Epoch: 15 / 30 Step: 2000\n",
      "Epoch 15 | train loss:0.4156 | test accuracy:0.4483\n",
      "Epoch: 16 / 30 Step: 0\n",
      "Epoch: 16 / 30 Step: 1000\n",
      "Epoch: 16 / 30 Step: 2000\n",
      "Epoch 16 | train loss:0.3801 | test accuracy:0.4446\n",
      "Epoch: 17 / 30 Step: 0\n",
      "Epoch: 17 / 30 Step: 1000\n",
      "Epoch: 17 / 30 Step: 2000\n",
      "Epoch 17 | train loss:0.4704 | test accuracy:0.4437\n",
      "Epoch: 18 / 30 Step: 0\n",
      "Epoch: 18 / 30 Step: 1000\n",
      "Epoch: 18 / 30 Step: 2000\n",
      "Epoch 18 | train loss:0.5831 | test accuracy:0.4439\n",
      "Epoch: 19 / 30 Step: 0\n",
      "Epoch: 19 / 30 Step: 1000\n",
      "Epoch: 19 / 30 Step: 2000\n",
      "Epoch 19 | train loss:0.7856 | test accuracy:0.4438\n",
      "Epoch: 20 / 30 Step: 0\n",
      "Epoch: 20 / 30 Step: 1000\n",
      "Epoch: 20 / 30 Step: 2000\n",
      "Epoch 20 | train loss:0.5208 | test accuracy:0.4442\n",
      "Epoch: 21 / 30 Step: 0\n",
      "Epoch: 21 / 30 Step: 1000\n",
      "Epoch: 21 / 30 Step: 2000\n",
      "Epoch 21 | train loss:0.3384 | test accuracy:0.4432\n",
      "Epoch: 22 / 30 Step: 0\n",
      "Epoch: 22 / 30 Step: 1000\n",
      "Epoch: 22 / 30 Step: 2000\n",
      "Epoch 22 | train loss:0.3944 | test accuracy:0.4435\n",
      "Epoch: 23 / 30 Step: 0\n",
      "Epoch: 23 / 30 Step: 1000\n",
      "Epoch: 23 / 30 Step: 2000\n",
      "Epoch 23 | train loss:0.4015 | test accuracy:0.4429\n",
      "Epoch: 24 / 30 Step: 0\n",
      "Epoch: 24 / 30 Step: 1000\n",
      "Epoch: 24 / 30 Step: 2000\n",
      "Epoch 24 | train loss:0.5017 | test accuracy:0.4429\n",
      "Epoch: 25 / 30 Step: 0\n",
      "Epoch: 25 / 30 Step: 1000\n",
      "Epoch: 25 / 30 Step: 2000\n",
      "Epoch 25 | train loss:0.6656 | test accuracy:0.4426\n",
      "Epoch: 26 / 30 Step: 0\n",
      "Epoch: 26 / 30 Step: 1000\n",
      "Epoch: 26 / 30 Step: 2000\n",
      "Epoch 26 | train loss:0.7909 | test accuracy:0.4426\n",
      "Epoch: 27 / 30 Step: 0\n",
      "Epoch: 27 / 30 Step: 1000\n",
      "Epoch: 27 / 30 Step: 2000\n",
      "Epoch 27 | train loss:0.6374 | test accuracy:0.4430\n",
      "Epoch: 28 / 30 Step: 0\n",
      "Epoch: 28 / 30 Step: 1000\n",
      "Epoch: 28 / 30 Step: 2000\n",
      "Epoch 28 | train loss:0.5523 | test accuracy:0.4434\n",
      "Epoch: 29 / 30 Step: 0\n",
      "Epoch: 29 / 30 Step: 1000\n",
      "Epoch: 29 / 30 Step: 2000\n",
      "Epoch 29 | train loss:0.3575 | test accuracy:0.4419\n",
      "Epoch: 30 / 30 Step: 0\n",
      "Epoch: 30 / 30 Step: 1000\n",
      "Epoch: 30 / 30 Step: 2000\n",
      "Epoch 30 | train loss:0.7422 | test accuracy:0.4427\n"
     ]
    }
   ],
   "source": [
    "i,j = 1,1\n",
    "td = tdata\n",
    "tl = tlabel\n",
    "ttd = sdata\n",
    "ttl = slabel\n",
    "cnn = CNN()\n",
    "cnn = train_model(cnn,td,tl,lr=0.01, batch_size=128, epoch=30)\n",
    "save_model(cnn, \"cnn_\"+str(i+1)+\"_\"+str(j+1)+\"_.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5996fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "误差： 1.8902924\n",
      "全误差： -233116.9\n",
      "总值： 241742.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.8902924"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(cnn, ttd, ttl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "236fed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = DenseGraphConv(in_channels, hidden_channels)\n",
    "        self.conv2 = DenseGraphConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: Tensor, adj: Tensor) -> Tensor:\n",
    "        x = self.conv1(x, adj).relu()\n",
    "        x = self.conv2(x, adj)\n",
    "        return x\n",
    "\n",
    "def save_model(net, path):\n",
    "    torch.save(net, path)\n",
    "\n",
    "def load_model(path):\n",
    "    net = torch.load(path)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45b83df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, data,label,ajmtx,lr, batch_size, epoch,early_stop = 10):\n",
    "    # net = net.cuda()\n",
    "    data = torch.Tensor(data)\n",
    "    ajmtx = torch.Tensor(ajmtx)\n",
    "    label = torch.Tensor(label).float()\n",
    "    # data =data.cuda()\n",
    "    # label=label.cuda()\n",
    "    \n",
    "    #train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.1, random_state=0)\n",
    "    \n",
    "    LR = lr\n",
    "    BATCH_SIZE = batch_size\n",
    "    EPOCH = epoch\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "    train_dataset = Data.TensorDataset(data,ajmtx,label)\n",
    "    train_loader = Data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    #test_dataset = Data.TensorDataset(test_data,aj_data,test_label)\n",
    "    test_loader = Data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, LR, epochs=EPOCH, steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    best = 1000\n",
    "    be = 0\n",
    "    for epoch in range(EPOCH):\n",
    "        for step, (batch_data,batch_aj,batch_label) in enumerate(train_loader):\n",
    "            if step % 100 == 0:\n",
    "                print('Epoch:', epoch + 1, '/', EPOCH, 'Step:', step)\n",
    "            prediction = net(batch_data,batch_aj).squeeze()\n",
    "            loss = F.mse_loss(prediction,batch_label)#F.cross_entropy(prediction, batch_label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        accuracy = []\n",
    "        for stp, (test_x,test_aj,test_y) in enumerate(test_loader):\n",
    "            test_output = net(test_x,test_aj).squeeze()\n",
    "            accuracy.append(np.mean(np.mean(abs(test_output-test_y).detach().numpy())))\n",
    "        succ = np.mean(accuracy)\n",
    "        if best > succ:\n",
    "            best = succ\n",
    "            save_model(net, \"tmp_gcn.pkl\")\n",
    "            be = epoch\n",
    "        if epoch - be >= early_stop:\n",
    "            net = load_model(\"tmp_gcn.pkl\")\n",
    "            print('Early Stop in Epoch', epoch + 1, '| train loss:%.4f' % loss, '| test accuracy:%.4f' % succ)\n",
    "            accuracy = []\n",
    "            for stp, (test_x,test_aj,test_y) in enumerate(test_loader):\n",
    "                test_output = net(test_x,test_aj).squeeze()\n",
    "                accuracy.append(np.mean(np.mean(abs(test_output-test_y).detach().numpy())))\n",
    "            succ = np.mean(accuracy)\n",
    "            print('Best Epoch', be + 1, '| test accuracy:%.4f' % succ)\n",
    "            return net\n",
    "        print('Epoch', epoch + 1, '| train loss:%.4f' % loss, '| test accuracy:%.4f' % succ)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81e4391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net,data,label,ajmtx):\n",
    "    test_data = torch.Tensor(data)\n",
    "    ajmtx = torch.Tensor(ajmtx)\n",
    "    test_label = torch.Tensor(label).float()\n",
    "    \n",
    "    test_dataset = Data.TensorDataset(test_data,ajmtx,test_label)\n",
    "    test_loader = Data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    ret = []\n",
    "    ret2 = []\n",
    "    ret3 = []\n",
    "    for stp, (test_x,test_aj,test_y) in enumerate(test_loader):\n",
    "        pred_y = net(test_x,test_aj).squeeze()\n",
    "        ret.append(np.mean(np.mean(abs(pred_y-test_y).detach().numpy())))\n",
    "        ret2.append(np.sum(np.sum((pred_y-test_y).detach().numpy())))\n",
    "        ret3.append(np.sum(np.sum((test_y).detach().numpy())))\n",
    "    #print(test_y)\n",
    "    #print(pred_y)\n",
    "    \n",
    "    print(\"误差：\",np.mean(ret))\n",
    "    print(\"全误差：\",np.sum(ret2))\n",
    "    print(\"总值：\",np.sum(ret3))\n",
    "    return np.mean(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60ea634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 300 Step: 0\n",
      "Epoch: 1 / 300 Step: 100\n",
      "Epoch: 1 / 300 Step: 200\n",
      "Epoch: 1 / 300 Step: 300\n",
      "Epoch: 1 / 300 Step: 400\n",
      "Epoch: 1 / 300 Step: 500\n",
      "Epoch: 1 / 300 Step: 600\n",
      "Epoch 1 | train loss:1.0700 | test accuracy:0.5858\n",
      "Epoch: 2 / 300 Step: 0\n",
      "Epoch: 2 / 300 Step: 100\n",
      "Epoch: 2 / 300 Step: 200\n",
      "Epoch: 2 / 300 Step: 300\n",
      "Epoch: 2 / 300 Step: 400\n",
      "Epoch: 2 / 300 Step: 500\n",
      "Epoch: 2 / 300 Step: 600\n",
      "Epoch 2 | train loss:1.1031 | test accuracy:0.5835\n",
      "Epoch: 3 / 300 Step: 0\n",
      "Epoch: 3 / 300 Step: 100\n",
      "Epoch: 3 / 300 Step: 200\n",
      "Epoch: 3 / 300 Step: 300\n",
      "Epoch: 3 / 300 Step: 400\n",
      "Epoch: 3 / 300 Step: 500\n",
      "Epoch: 3 / 300 Step: 600\n",
      "Epoch 3 | train loss:0.8024 | test accuracy:0.5129\n",
      "Epoch: 4 / 300 Step: 0\n",
      "Epoch: 4 / 300 Step: 100\n",
      "Epoch: 4 / 300 Step: 200\n",
      "Epoch: 4 / 300 Step: 300\n",
      "Epoch: 4 / 300 Step: 400\n",
      "Epoch: 4 / 300 Step: 500\n",
      "Epoch: 4 / 300 Step: 600\n",
      "Epoch 4 | train loss:1.1399 | test accuracy:0.5024\n",
      "Epoch: 5 / 300 Step: 0\n",
      "Epoch: 5 / 300 Step: 100\n",
      "Epoch: 5 / 300 Step: 200\n",
      "Epoch: 5 / 300 Step: 300\n",
      "Epoch: 5 / 300 Step: 400\n",
      "Epoch: 5 / 300 Step: 500\n",
      "Epoch: 5 / 300 Step: 600\n",
      "Epoch 5 | train loss:0.3899 | test accuracy:0.5045\n",
      "Epoch: 6 / 300 Step: 0\n",
      "Epoch: 6 / 300 Step: 100\n",
      "Epoch: 6 / 300 Step: 200\n",
      "Epoch: 6 / 300 Step: 300\n",
      "Epoch: 6 / 300 Step: 400\n",
      "Epoch: 6 / 300 Step: 500\n",
      "Epoch: 6 / 300 Step: 600\n",
      "Epoch 6 | train loss:0.7706 | test accuracy:0.4906\n",
      "Epoch: 7 / 300 Step: 0\n",
      "Epoch: 7 / 300 Step: 100\n",
      "Epoch: 7 / 300 Step: 200\n",
      "Epoch: 7 / 300 Step: 300\n",
      "Epoch: 7 / 300 Step: 400\n",
      "Epoch: 7 / 300 Step: 500\n",
      "Epoch: 7 / 300 Step: 600\n",
      "Epoch 7 | train loss:1.0299 | test accuracy:0.4948\n",
      "Epoch: 8 / 300 Step: 0\n",
      "Epoch: 8 / 300 Step: 100\n",
      "Epoch: 8 / 300 Step: 200\n",
      "Epoch: 8 / 300 Step: 300\n",
      "Epoch: 8 / 300 Step: 400\n",
      "Epoch: 8 / 300 Step: 500\n",
      "Epoch: 8 / 300 Step: 600\n",
      "Epoch 8 | train loss:0.8115 | test accuracy:0.4904\n",
      "Epoch: 9 / 300 Step: 0\n",
      "Epoch: 9 / 300 Step: 100\n",
      "Epoch: 9 / 300 Step: 200\n",
      "Epoch: 9 / 300 Step: 300\n",
      "Epoch: 9 / 300 Step: 400\n",
      "Epoch: 9 / 300 Step: 500\n",
      "Epoch: 9 / 300 Step: 600\n",
      "Epoch 9 | train loss:1.1071 | test accuracy:0.4508\n",
      "Epoch: 10 / 300 Step: 0\n",
      "Epoch: 10 / 300 Step: 100\n",
      "Epoch: 10 / 300 Step: 200\n",
      "Epoch: 10 / 300 Step: 300\n",
      "Epoch: 10 / 300 Step: 400\n",
      "Epoch: 10 / 300 Step: 500\n",
      "Epoch: 10 / 300 Step: 600\n",
      "Epoch 10 | train loss:0.2887 | test accuracy:0.4470\n",
      "Epoch: 11 / 300 Step: 0\n",
      "Epoch: 11 / 300 Step: 100\n",
      "Epoch: 11 / 300 Step: 200\n",
      "Epoch: 11 / 300 Step: 300\n",
      "Epoch: 11 / 300 Step: 400\n",
      "Epoch: 11 / 300 Step: 500\n",
      "Epoch: 11 / 300 Step: 600\n",
      "Epoch 11 | train loss:0.5851 | test accuracy:0.4402\n",
      "Epoch: 12 / 300 Step: 0\n",
      "Epoch: 12 / 300 Step: 100\n",
      "Epoch: 12 / 300 Step: 200\n",
      "Epoch: 12 / 300 Step: 300\n",
      "Epoch: 12 / 300 Step: 400\n",
      "Epoch: 12 / 300 Step: 500\n",
      "Epoch: 12 / 300 Step: 600\n",
      "Epoch 12 | train loss:0.6396 | test accuracy:0.4413\n",
      "Epoch: 13 / 300 Step: 0\n",
      "Epoch: 13 / 300 Step: 100\n",
      "Epoch: 13 / 300 Step: 200\n",
      "Epoch: 13 / 300 Step: 300\n",
      "Epoch: 13 / 300 Step: 400\n",
      "Epoch: 13 / 300 Step: 500\n",
      "Epoch: 13 / 300 Step: 600\n",
      "Epoch 13 | train loss:0.4250 | test accuracy:0.4312\n",
      "Epoch: 14 / 300 Step: 0\n",
      "Epoch: 14 / 300 Step: 100\n",
      "Epoch: 14 / 300 Step: 200\n",
      "Epoch: 14 / 300 Step: 300\n",
      "Epoch: 14 / 300 Step: 400\n",
      "Epoch: 14 / 300 Step: 500\n",
      "Epoch: 14 / 300 Step: 600\n",
      "Epoch 14 | train loss:0.3080 | test accuracy:0.4103\n",
      "Epoch: 15 / 300 Step: 0\n",
      "Epoch: 15 / 300 Step: 100\n",
      "Epoch: 15 / 300 Step: 200\n",
      "Epoch: 15 / 300 Step: 300\n",
      "Epoch: 15 / 300 Step: 400\n",
      "Epoch: 15 / 300 Step: 500\n",
      "Epoch: 15 / 300 Step: 600\n",
      "Epoch 15 | train loss:0.6070 | test accuracy:0.4225\n",
      "Epoch: 16 / 300 Step: 0\n",
      "Epoch: 16 / 300 Step: 100\n",
      "Epoch: 16 / 300 Step: 200\n",
      "Epoch: 16 / 300 Step: 300\n",
      "Epoch: 16 / 300 Step: 400\n",
      "Epoch: 16 / 300 Step: 500\n",
      "Epoch: 16 / 300 Step: 600\n",
      "Epoch 16 | train loss:0.4811 | test accuracy:0.3989\n",
      "Epoch: 17 / 300 Step: 0\n",
      "Epoch: 17 / 300 Step: 100\n",
      "Epoch: 17 / 300 Step: 200\n",
      "Epoch: 17 / 300 Step: 300\n",
      "Epoch: 17 / 300 Step: 400\n",
      "Epoch: 17 / 300 Step: 500\n",
      "Epoch: 17 / 300 Step: 600\n",
      "Epoch 17 | train loss:0.3523 | test accuracy:0.4004\n",
      "Epoch: 18 / 300 Step: 0\n",
      "Epoch: 18 / 300 Step: 100\n",
      "Epoch: 18 / 300 Step: 200\n",
      "Epoch: 18 / 300 Step: 300\n",
      "Epoch: 18 / 300 Step: 400\n",
      "Epoch: 18 / 300 Step: 500\n",
      "Epoch: 18 / 300 Step: 600\n",
      "Epoch 18 | train loss:0.5835 | test accuracy:0.3995\n",
      "Epoch: 19 / 300 Step: 0\n",
      "Epoch: 19 / 300 Step: 100\n",
      "Epoch: 19 / 300 Step: 200\n",
      "Epoch: 19 / 300 Step: 300\n",
      "Epoch: 19 / 300 Step: 400\n",
      "Epoch: 19 / 300 Step: 500\n",
      "Epoch: 19 / 300 Step: 600\n",
      "Epoch 19 | train loss:0.4602 | test accuracy:0.3863\n",
      "Epoch: 20 / 300 Step: 0\n",
      "Epoch: 20 / 300 Step: 100\n",
      "Epoch: 20 / 300 Step: 200\n",
      "Epoch: 20 / 300 Step: 300\n",
      "Epoch: 20 / 300 Step: 400\n",
      "Epoch: 20 / 300 Step: 500\n",
      "Epoch: 20 / 300 Step: 600\n",
      "Epoch 20 | train loss:0.3894 | test accuracy:0.3960\n",
      "Epoch: 21 / 300 Step: 0\n",
      "Epoch: 21 / 300 Step: 100\n",
      "Epoch: 21 / 300 Step: 200\n",
      "Epoch: 21 / 300 Step: 300\n",
      "Epoch: 21 / 300 Step: 400\n",
      "Epoch: 21 / 300 Step: 500\n",
      "Epoch: 21 / 300 Step: 600\n",
      "Epoch 21 | train loss:0.5983 | test accuracy:0.3835\n",
      "Epoch: 22 / 300 Step: 0\n",
      "Epoch: 22 / 300 Step: 100\n",
      "Epoch: 22 / 300 Step: 200\n",
      "Epoch: 22 / 300 Step: 300\n",
      "Epoch: 22 / 300 Step: 400\n",
      "Epoch: 22 / 300 Step: 500\n",
      "Epoch: 22 / 300 Step: 600\n",
      "Epoch 22 | train loss:0.2891 | test accuracy:0.3955\n",
      "Epoch: 23 / 300 Step: 0\n",
      "Epoch: 23 / 300 Step: 100\n",
      "Epoch: 23 / 300 Step: 200\n",
      "Epoch: 23 / 300 Step: 300\n",
      "Epoch: 23 / 300 Step: 400\n",
      "Epoch: 23 / 300 Step: 500\n",
      "Epoch: 23 / 300 Step: 600\n",
      "Epoch 23 | train loss:0.7306 | test accuracy:0.3785\n",
      "Epoch: 24 / 300 Step: 0\n",
      "Epoch: 24 / 300 Step: 100\n",
      "Epoch: 24 / 300 Step: 200\n",
      "Epoch: 24 / 300 Step: 300\n",
      "Epoch: 24 / 300 Step: 400\n",
      "Epoch: 24 / 300 Step: 500\n",
      "Epoch: 24 / 300 Step: 600\n",
      "Epoch 24 | train loss:0.3832 | test accuracy:0.3710\n",
      "Epoch: 25 / 300 Step: 0\n",
      "Epoch: 25 / 300 Step: 100\n",
      "Epoch: 25 / 300 Step: 200\n",
      "Epoch: 25 / 300 Step: 300\n",
      "Epoch: 25 / 300 Step: 400\n",
      "Epoch: 25 / 300 Step: 500\n",
      "Epoch: 25 / 300 Step: 600\n",
      "Epoch 25 | train loss:0.4659 | test accuracy:0.3836\n",
      "Epoch: 26 / 300 Step: 0\n",
      "Epoch: 26 / 300 Step: 100\n",
      "Epoch: 26 / 300 Step: 200\n",
      "Epoch: 26 / 300 Step: 300\n",
      "Epoch: 26 / 300 Step: 400\n",
      "Epoch: 26 / 300 Step: 500\n",
      "Epoch: 26 / 300 Step: 600\n",
      "Epoch 26 | train loss:0.4308 | test accuracy:0.3598\n",
      "Epoch: 27 / 300 Step: 0\n",
      "Epoch: 27 / 300 Step: 100\n",
      "Epoch: 27 / 300 Step: 200\n",
      "Epoch: 27 / 300 Step: 300\n",
      "Epoch: 27 / 300 Step: 400\n",
      "Epoch: 27 / 300 Step: 500\n",
      "Epoch: 27 / 300 Step: 600\n",
      "Epoch 27 | train loss:0.7669 | test accuracy:0.3797\n",
      "Epoch: 28 / 300 Step: 0\n",
      "Epoch: 28 / 300 Step: 100\n",
      "Epoch: 28 / 300 Step: 200\n",
      "Epoch: 28 / 300 Step: 300\n",
      "Epoch: 28 / 300 Step: 400\n",
      "Epoch: 28 / 300 Step: 500\n",
      "Epoch: 28 / 300 Step: 600\n",
      "Epoch 28 | train loss:0.4066 | test accuracy:0.3656\n",
      "Epoch: 29 / 300 Step: 0\n",
      "Epoch: 29 / 300 Step: 100\n",
      "Epoch: 29 / 300 Step: 200\n",
      "Epoch: 29 / 300 Step: 300\n",
      "Epoch: 29 / 300 Step: 400\n",
      "Epoch: 29 / 300 Step: 500\n",
      "Epoch: 29 / 300 Step: 600\n",
      "Epoch 29 | train loss:0.2341 | test accuracy:0.3691\n",
      "Epoch: 30 / 300 Step: 0\n",
      "Epoch: 30 / 300 Step: 100\n",
      "Epoch: 30 / 300 Step: 200\n",
      "Epoch: 30 / 300 Step: 300\n",
      "Epoch: 30 / 300 Step: 400\n",
      "Epoch: 30 / 300 Step: 500\n",
      "Epoch: 30 / 300 Step: 600\n",
      "Epoch 30 | train loss:0.5836 | test accuracy:0.3589\n",
      "Epoch: 31 / 300 Step: 0\n",
      "Epoch: 31 / 300 Step: 100\n",
      "Epoch: 31 / 300 Step: 200\n",
      "Epoch: 31 / 300 Step: 300\n",
      "Epoch: 31 / 300 Step: 400\n",
      "Epoch: 31 / 300 Step: 500\n",
      "Epoch: 31 / 300 Step: 600\n",
      "Epoch 31 | train loss:0.6317 | test accuracy:0.3661\n",
      "Epoch: 32 / 300 Step: 0\n",
      "Epoch: 32 / 300 Step: 100\n",
      "Epoch: 32 / 300 Step: 200\n",
      "Epoch: 32 / 300 Step: 300\n",
      "Epoch: 32 / 300 Step: 400\n",
      "Epoch: 32 / 300 Step: 500\n",
      "Epoch: 32 / 300 Step: 600\n",
      "Epoch 32 | train loss:0.3370 | test accuracy:0.3453\n",
      "Epoch: 33 / 300 Step: 0\n",
      "Epoch: 33 / 300 Step: 100\n",
      "Epoch: 33 / 300 Step: 200\n",
      "Epoch: 33 / 300 Step: 300\n",
      "Epoch: 33 / 300 Step: 400\n",
      "Epoch: 33 / 300 Step: 500\n",
      "Epoch: 33 / 300 Step: 600\n",
      "Epoch 33 | train loss:0.3407 | test accuracy:0.3577\n",
      "Epoch: 34 / 300 Step: 0\n",
      "Epoch: 34 / 300 Step: 100\n",
      "Epoch: 34 / 300 Step: 200\n",
      "Epoch: 34 / 300 Step: 300\n",
      "Epoch: 34 / 300 Step: 400\n",
      "Epoch: 34 / 300 Step: 500\n",
      "Epoch: 34 / 300 Step: 600\n",
      "Epoch 34 | train loss:0.4312 | test accuracy:0.3594\n",
      "Epoch: 35 / 300 Step: 0\n",
      "Epoch: 35 / 300 Step: 100\n",
      "Epoch: 35 / 300 Step: 200\n",
      "Epoch: 35 / 300 Step: 300\n",
      "Epoch: 35 / 300 Step: 400\n",
      "Epoch: 35 / 300 Step: 500\n",
      "Epoch: 35 / 300 Step: 600\n",
      "Epoch 35 | train loss:0.6977 | test accuracy:0.3830\n",
      "Epoch: 36 / 300 Step: 0\n",
      "Epoch: 36 / 300 Step: 100\n",
      "Epoch: 36 / 300 Step: 200\n",
      "Epoch: 36 / 300 Step: 300\n",
      "Epoch: 36 / 300 Step: 400\n",
      "Epoch: 36 / 300 Step: 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 / 300 Step: 600\n",
      "Epoch 36 | train loss:0.3532 | test accuracy:0.3596\n",
      "Epoch: 37 / 300 Step: 0\n",
      "Epoch: 37 / 300 Step: 100\n",
      "Epoch: 37 / 300 Step: 200\n",
      "Epoch: 37 / 300 Step: 300\n",
      "Epoch: 37 / 300 Step: 400\n",
      "Epoch: 37 / 300 Step: 500\n",
      "Epoch: 37 / 300 Step: 600\n",
      "Epoch 37 | train loss:0.5233 | test accuracy:0.3435\n",
      "Epoch: 38 / 300 Step: 0\n",
      "Epoch: 38 / 300 Step: 100\n",
      "Epoch: 38 / 300 Step: 200\n",
      "Epoch: 38 / 300 Step: 300\n",
      "Epoch: 38 / 300 Step: 400\n",
      "Epoch: 38 / 300 Step: 500\n",
      "Epoch: 38 / 300 Step: 600\n",
      "Epoch 38 | train loss:0.2778 | test accuracy:0.3612\n",
      "Epoch: 39 / 300 Step: 0\n",
      "Epoch: 39 / 300 Step: 100\n",
      "Epoch: 39 / 300 Step: 200\n",
      "Epoch: 39 / 300 Step: 300\n",
      "Epoch: 39 / 300 Step: 400\n",
      "Epoch: 39 / 300 Step: 500\n",
      "Epoch: 39 / 300 Step: 600\n",
      "Epoch 39 | train loss:0.9953 | test accuracy:0.3559\n",
      "Epoch: 40 / 300 Step: 0\n",
      "Epoch: 40 / 300 Step: 100\n",
      "Epoch: 40 / 300 Step: 200\n",
      "Epoch: 40 / 300 Step: 300\n",
      "Epoch: 40 / 300 Step: 400\n",
      "Epoch: 40 / 300 Step: 500\n",
      "Epoch: 40 / 300 Step: 600\n",
      "Epoch 40 | train loss:0.4496 | test accuracy:0.3723\n",
      "Epoch: 41 / 300 Step: 0\n",
      "Epoch: 41 / 300 Step: 100\n",
      "Epoch: 41 / 300 Step: 200\n",
      "Epoch: 41 / 300 Step: 300\n",
      "Epoch: 41 / 300 Step: 400\n",
      "Epoch: 41 / 300 Step: 500\n",
      "Epoch: 41 / 300 Step: 600\n",
      "Epoch 41 | train loss:0.3452 | test accuracy:0.3475\n",
      "Epoch: 42 / 300 Step: 0\n",
      "Epoch: 42 / 300 Step: 100\n",
      "Epoch: 42 / 300 Step: 200\n",
      "Epoch: 42 / 300 Step: 300\n",
      "Epoch: 42 / 300 Step: 400\n",
      "Epoch: 42 / 300 Step: 500\n",
      "Epoch: 42 / 300 Step: 600\n",
      "Epoch 42 | train loss:0.4004 | test accuracy:0.3540\n",
      "Epoch: 43 / 300 Step: 0\n",
      "Epoch: 43 / 300 Step: 100\n",
      "Epoch: 43 / 300 Step: 200\n",
      "Epoch: 43 / 300 Step: 300\n",
      "Epoch: 43 / 300 Step: 400\n",
      "Epoch: 43 / 300 Step: 500\n",
      "Epoch: 43 / 300 Step: 600\n",
      "Epoch 43 | train loss:0.4445 | test accuracy:0.3507\n",
      "Epoch: 44 / 300 Step: 0\n",
      "Epoch: 44 / 300 Step: 100\n",
      "Epoch: 44 / 300 Step: 200\n",
      "Epoch: 44 / 300 Step: 300\n",
      "Epoch: 44 / 300 Step: 400\n",
      "Epoch: 44 / 300 Step: 500\n",
      "Epoch: 44 / 300 Step: 600\n",
      "Epoch 44 | train loss:0.5075 | test accuracy:0.3611\n",
      "Epoch: 45 / 300 Step: 0\n",
      "Epoch: 45 / 300 Step: 100\n",
      "Epoch: 45 / 300 Step: 200\n",
      "Epoch: 45 / 300 Step: 300\n",
      "Epoch: 45 / 300 Step: 400\n",
      "Epoch: 45 / 300 Step: 500\n",
      "Epoch: 45 / 300 Step: 600\n",
      "Epoch 45 | train loss:0.3186 | test accuracy:0.3429\n",
      "Epoch: 46 / 300 Step: 0\n",
      "Epoch: 46 / 300 Step: 100\n",
      "Epoch: 46 / 300 Step: 200\n",
      "Epoch: 46 / 300 Step: 300\n",
      "Epoch: 46 / 300 Step: 400\n",
      "Epoch: 46 / 300 Step: 500\n",
      "Epoch: 46 / 300 Step: 600\n",
      "Epoch 46 | train loss:0.4746 | test accuracy:0.3261\n",
      "Epoch: 47 / 300 Step: 0\n",
      "Epoch: 47 / 300 Step: 100\n",
      "Epoch: 47 / 300 Step: 200\n",
      "Epoch: 47 / 300 Step: 300\n",
      "Epoch: 47 / 300 Step: 400\n",
      "Epoch: 47 / 300 Step: 500\n",
      "Epoch: 47 / 300 Step: 600\n",
      "Epoch 47 | train loss:0.3786 | test accuracy:0.3422\n",
      "Epoch: 48 / 300 Step: 0\n",
      "Epoch: 48 / 300 Step: 100\n",
      "Epoch: 48 / 300 Step: 200\n",
      "Epoch: 48 / 300 Step: 300\n",
      "Epoch: 48 / 300 Step: 400\n",
      "Epoch: 48 / 300 Step: 500\n",
      "Epoch: 48 / 300 Step: 600\n",
      "Epoch 48 | train loss:0.2190 | test accuracy:0.3256\n",
      "Epoch: 49 / 300 Step: 0\n",
      "Epoch: 49 / 300 Step: 100\n",
      "Epoch: 49 / 300 Step: 200\n",
      "Epoch: 49 / 300 Step: 300\n",
      "Epoch: 49 / 300 Step: 400\n",
      "Epoch: 49 / 300 Step: 500\n",
      "Epoch: 49 / 300 Step: 600\n",
      "Epoch 49 | train loss:0.7155 | test accuracy:0.3277\n",
      "Epoch: 50 / 300 Step: 0\n",
      "Epoch: 50 / 300 Step: 100\n",
      "Epoch: 50 / 300 Step: 200\n",
      "Epoch: 50 / 300 Step: 300\n",
      "Epoch: 50 / 300 Step: 400\n",
      "Epoch: 50 / 300 Step: 500\n",
      "Epoch: 50 / 300 Step: 600\n",
      "Epoch 50 | train loss:0.2206 | test accuracy:0.3470\n",
      "Epoch: 51 / 300 Step: 0\n",
      "Epoch: 51 / 300 Step: 100\n",
      "Epoch: 51 / 300 Step: 200\n",
      "Epoch: 51 / 300 Step: 300\n",
      "Epoch: 51 / 300 Step: 400\n",
      "Epoch: 51 / 300 Step: 500\n",
      "Epoch: 51 / 300 Step: 600\n",
      "Epoch 51 | train loss:0.5262 | test accuracy:0.3365\n",
      "Epoch: 52 / 300 Step: 0\n",
      "Epoch: 52 / 300 Step: 100\n",
      "Epoch: 52 / 300 Step: 200\n",
      "Epoch: 52 / 300 Step: 300\n",
      "Epoch: 52 / 300 Step: 400\n",
      "Epoch: 52 / 300 Step: 500\n",
      "Epoch: 52 / 300 Step: 600\n",
      "Epoch 52 | train loss:0.2504 | test accuracy:0.3378\n",
      "Epoch: 53 / 300 Step: 0\n",
      "Epoch: 53 / 300 Step: 100\n",
      "Epoch: 53 / 300 Step: 200\n",
      "Epoch: 53 / 300 Step: 300\n",
      "Epoch: 53 / 300 Step: 400\n",
      "Epoch: 53 / 300 Step: 500\n",
      "Epoch: 53 / 300 Step: 600\n",
      "Epoch 53 | train loss:0.2975 | test accuracy:0.3360\n",
      "Epoch: 54 / 300 Step: 0\n",
      "Epoch: 54 / 300 Step: 100\n",
      "Epoch: 54 / 300 Step: 200\n",
      "Epoch: 54 / 300 Step: 300\n",
      "Epoch: 54 / 300 Step: 400\n",
      "Epoch: 54 / 300 Step: 500\n",
      "Epoch: 54 / 300 Step: 600\n",
      "Epoch 54 | train loss:0.2737 | test accuracy:0.3411\n",
      "Epoch: 55 / 300 Step: 0\n",
      "Epoch: 55 / 300 Step: 100\n",
      "Epoch: 55 / 300 Step: 200\n",
      "Epoch: 55 / 300 Step: 300\n",
      "Epoch: 55 / 300 Step: 400\n",
      "Epoch: 55 / 300 Step: 500\n",
      "Epoch: 55 / 300 Step: 600\n",
      "Epoch 55 | train loss:0.2178 | test accuracy:0.3250\n",
      "Epoch: 56 / 300 Step: 0\n",
      "Epoch: 56 / 300 Step: 100\n",
      "Epoch: 56 / 300 Step: 200\n",
      "Epoch: 56 / 300 Step: 300\n",
      "Epoch: 56 / 300 Step: 400\n",
      "Epoch: 56 / 300 Step: 500\n",
      "Epoch: 56 / 300 Step: 600\n",
      "Epoch 56 | train loss:0.3287 | test accuracy:0.3352\n",
      "Epoch: 57 / 300 Step: 0\n",
      "Epoch: 57 / 300 Step: 100\n",
      "Epoch: 57 / 300 Step: 200\n",
      "Epoch: 57 / 300 Step: 300\n",
      "Epoch: 57 / 300 Step: 400\n",
      "Epoch: 57 / 300 Step: 500\n",
      "Epoch: 57 / 300 Step: 600\n",
      "Epoch 57 | train loss:0.1858 | test accuracy:0.3362\n",
      "Epoch: 58 / 300 Step: 0\n",
      "Epoch: 58 / 300 Step: 100\n",
      "Epoch: 58 / 300 Step: 200\n",
      "Epoch: 58 / 300 Step: 300\n",
      "Epoch: 58 / 300 Step: 400\n",
      "Epoch: 58 / 300 Step: 500\n",
      "Epoch: 58 / 300 Step: 600\n",
      "Epoch 58 | train loss:0.2944 | test accuracy:0.3221\n",
      "Epoch: 59 / 300 Step: 0\n",
      "Epoch: 59 / 300 Step: 100\n",
      "Epoch: 59 / 300 Step: 200\n",
      "Epoch: 59 / 300 Step: 300\n",
      "Epoch: 59 / 300 Step: 400\n",
      "Epoch: 59 / 300 Step: 500\n",
      "Epoch: 59 / 300 Step: 600\n",
      "Epoch 59 | train loss:0.3540 | test accuracy:0.3270\n",
      "Epoch: 60 / 300 Step: 0\n",
      "Epoch: 60 / 300 Step: 100\n",
      "Epoch: 60 / 300 Step: 200\n",
      "Epoch: 60 / 300 Step: 300\n",
      "Epoch: 60 / 300 Step: 400\n",
      "Epoch: 60 / 300 Step: 500\n",
      "Epoch: 60 / 300 Step: 600\n",
      "Epoch 60 | train loss:0.3864 | test accuracy:0.3241\n",
      "Epoch: 61 / 300 Step: 0\n",
      "Epoch: 61 / 300 Step: 100\n",
      "Epoch: 61 / 300 Step: 200\n",
      "Epoch: 61 / 300 Step: 300\n",
      "Epoch: 61 / 300 Step: 400\n",
      "Epoch: 61 / 300 Step: 500\n",
      "Epoch: 61 / 300 Step: 600\n",
      "Epoch 61 | train loss:0.3291 | test accuracy:0.3264\n",
      "Epoch: 62 / 300 Step: 0\n",
      "Epoch: 62 / 300 Step: 100\n",
      "Epoch: 62 / 300 Step: 200\n",
      "Epoch: 62 / 300 Step: 300\n",
      "Epoch: 62 / 300 Step: 400\n",
      "Epoch: 62 / 300 Step: 500\n",
      "Epoch: 62 / 300 Step: 600\n",
      "Epoch 62 | train loss:0.2365 | test accuracy:0.3399\n",
      "Epoch: 63 / 300 Step: 0\n",
      "Epoch: 63 / 300 Step: 100\n",
      "Epoch: 63 / 300 Step: 200\n",
      "Epoch: 63 / 300 Step: 300\n",
      "Epoch: 63 / 300 Step: 400\n",
      "Epoch: 63 / 300 Step: 500\n",
      "Epoch: 63 / 300 Step: 600\n",
      "Epoch 63 | train loss:0.3923 | test accuracy:0.3255\n",
      "Epoch: 64 / 300 Step: 0\n",
      "Epoch: 64 / 300 Step: 100\n",
      "Epoch: 64 / 300 Step: 200\n",
      "Epoch: 64 / 300 Step: 300\n",
      "Epoch: 64 / 300 Step: 400\n",
      "Epoch: 64 / 300 Step: 500\n",
      "Epoch: 64 / 300 Step: 600\n",
      "Epoch 64 | train loss:0.3363 | test accuracy:0.3486\n",
      "Epoch: 65 / 300 Step: 0\n",
      "Epoch: 65 / 300 Step: 100\n",
      "Epoch: 65 / 300 Step: 200\n",
      "Epoch: 65 / 300 Step: 300\n",
      "Epoch: 65 / 300 Step: 400\n",
      "Epoch: 65 / 300 Step: 500\n",
      "Epoch: 65 / 300 Step: 600\n",
      "Epoch 65 | train loss:0.2585 | test accuracy:0.3127\n",
      "Epoch: 66 / 300 Step: 0\n",
      "Epoch: 66 / 300 Step: 100\n",
      "Epoch: 66 / 300 Step: 200\n",
      "Epoch: 66 / 300 Step: 300\n",
      "Epoch: 66 / 300 Step: 400\n",
      "Epoch: 66 / 300 Step: 500\n",
      "Epoch: 66 / 300 Step: 600\n",
      "Epoch 66 | train loss:0.2713 | test accuracy:0.3247\n",
      "Epoch: 67 / 300 Step: 0\n",
      "Epoch: 67 / 300 Step: 100\n",
      "Epoch: 67 / 300 Step: 200\n",
      "Epoch: 67 / 300 Step: 300\n",
      "Epoch: 67 / 300 Step: 400\n",
      "Epoch: 67 / 300 Step: 500\n",
      "Epoch: 67 / 300 Step: 600\n",
      "Epoch 67 | train loss:0.3830 | test accuracy:0.3326\n",
      "Epoch: 68 / 300 Step: 0\n",
      "Epoch: 68 / 300 Step: 100\n",
      "Epoch: 68 / 300 Step: 200\n",
      "Epoch: 68 / 300 Step: 300\n",
      "Epoch: 68 / 300 Step: 400\n",
      "Epoch: 68 / 300 Step: 500\n",
      "Epoch: 68 / 300 Step: 600\n",
      "Epoch 68 | train loss:0.3040 | test accuracy:0.3104\n",
      "Epoch: 69 / 300 Step: 0\n",
      "Epoch: 69 / 300 Step: 100\n",
      "Epoch: 69 / 300 Step: 200\n",
      "Epoch: 69 / 300 Step: 300\n",
      "Epoch: 69 / 300 Step: 400\n",
      "Epoch: 69 / 300 Step: 500\n",
      "Epoch: 69 / 300 Step: 600\n",
      "Epoch 69 | train loss:0.2717 | test accuracy:0.3184\n",
      "Epoch: 70 / 300 Step: 0\n",
      "Epoch: 70 / 300 Step: 100\n",
      "Epoch: 70 / 300 Step: 200\n",
      "Epoch: 70 / 300 Step: 300\n",
      "Epoch: 70 / 300 Step: 400\n",
      "Epoch: 70 / 300 Step: 500\n",
      "Epoch: 70 / 300 Step: 600\n",
      "Epoch 70 | train loss:0.2751 | test accuracy:0.3053\n",
      "Epoch: 71 / 300 Step: 0\n",
      "Epoch: 71 / 300 Step: 100\n",
      "Epoch: 71 / 300 Step: 200\n",
      "Epoch: 71 / 300 Step: 300\n",
      "Epoch: 71 / 300 Step: 400\n",
      "Epoch: 71 / 300 Step: 500\n",
      "Epoch: 71 / 300 Step: 600\n",
      "Epoch 71 | train loss:0.4094 | test accuracy:0.3364\n",
      "Epoch: 72 / 300 Step: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 / 300 Step: 100\n",
      "Epoch: 72 / 300 Step: 200\n",
      "Epoch: 72 / 300 Step: 300\n",
      "Epoch: 72 / 300 Step: 400\n",
      "Epoch: 72 / 300 Step: 500\n",
      "Epoch: 72 / 300 Step: 600\n",
      "Epoch 72 | train loss:0.2699 | test accuracy:0.3143\n",
      "Epoch: 73 / 300 Step: 0\n",
      "Epoch: 73 / 300 Step: 100\n",
      "Epoch: 73 / 300 Step: 200\n",
      "Epoch: 73 / 300 Step: 300\n",
      "Epoch: 73 / 300 Step: 400\n",
      "Epoch: 73 / 300 Step: 500\n",
      "Epoch: 73 / 300 Step: 600\n",
      "Epoch 73 | train loss:0.4186 | test accuracy:0.3183\n",
      "Epoch: 74 / 300 Step: 0\n",
      "Epoch: 74 / 300 Step: 100\n",
      "Epoch: 74 / 300 Step: 200\n",
      "Epoch: 74 / 300 Step: 300\n",
      "Epoch: 74 / 300 Step: 400\n",
      "Epoch: 74 / 300 Step: 500\n",
      "Epoch: 74 / 300 Step: 600\n",
      "Epoch 74 | train loss:0.3681 | test accuracy:0.3321\n",
      "Epoch: 75 / 300 Step: 0\n",
      "Epoch: 75 / 300 Step: 100\n",
      "Epoch: 75 / 300 Step: 200\n",
      "Epoch: 75 / 300 Step: 300\n",
      "Epoch: 75 / 300 Step: 400\n",
      "Epoch: 75 / 300 Step: 500\n",
      "Epoch: 75 / 300 Step: 600\n",
      "Epoch 75 | train loss:0.3867 | test accuracy:0.3128\n",
      "Epoch: 76 / 300 Step: 0\n",
      "Epoch: 76 / 300 Step: 100\n",
      "Epoch: 76 / 300 Step: 200\n",
      "Epoch: 76 / 300 Step: 300\n",
      "Epoch: 76 / 300 Step: 400\n",
      "Epoch: 76 / 300 Step: 500\n",
      "Epoch: 76 / 300 Step: 600\n",
      "Epoch 76 | train loss:0.2732 | test accuracy:0.3118\n",
      "Epoch: 77 / 300 Step: 0\n",
      "Epoch: 77 / 300 Step: 100\n",
      "Epoch: 77 / 300 Step: 200\n",
      "Epoch: 77 / 300 Step: 300\n",
      "Epoch: 77 / 300 Step: 400\n",
      "Epoch: 77 / 300 Step: 500\n",
      "Epoch: 77 / 300 Step: 600\n",
      "Epoch 77 | train loss:0.1726 | test accuracy:0.2963\n",
      "Epoch: 78 / 300 Step: 0\n",
      "Epoch: 78 / 300 Step: 100\n",
      "Epoch: 78 / 300 Step: 200\n",
      "Epoch: 78 / 300 Step: 300\n",
      "Epoch: 78 / 300 Step: 400\n",
      "Epoch: 78 / 300 Step: 500\n",
      "Epoch: 78 / 300 Step: 600\n",
      "Epoch 78 | train loss:0.2198 | test accuracy:0.3128\n",
      "Epoch: 79 / 300 Step: 0\n",
      "Epoch: 79 / 300 Step: 100\n",
      "Epoch: 79 / 300 Step: 200\n",
      "Epoch: 79 / 300 Step: 300\n",
      "Epoch: 79 / 300 Step: 400\n",
      "Epoch: 79 / 300 Step: 500\n",
      "Epoch: 79 / 300 Step: 600\n",
      "Epoch 79 | train loss:0.1401 | test accuracy:0.3097\n",
      "Epoch: 80 / 300 Step: 0\n",
      "Epoch: 80 / 300 Step: 100\n",
      "Epoch: 80 / 300 Step: 200\n",
      "Epoch: 80 / 300 Step: 300\n",
      "Epoch: 80 / 300 Step: 400\n",
      "Epoch: 80 / 300 Step: 500\n",
      "Epoch: 80 / 300 Step: 600\n",
      "Epoch 80 | train loss:0.2554 | test accuracy:0.3064\n",
      "Epoch: 81 / 300 Step: 0\n",
      "Epoch: 81 / 300 Step: 100\n",
      "Epoch: 81 / 300 Step: 200\n",
      "Epoch: 81 / 300 Step: 300\n",
      "Epoch: 81 / 300 Step: 400\n",
      "Epoch: 81 / 300 Step: 500\n",
      "Epoch: 81 / 300 Step: 600\n",
      "Epoch 81 | train loss:0.3618 | test accuracy:0.3131\n",
      "Epoch: 82 / 300 Step: 0\n",
      "Epoch: 82 / 300 Step: 100\n",
      "Epoch: 82 / 300 Step: 200\n",
      "Epoch: 82 / 300 Step: 300\n",
      "Epoch: 82 / 300 Step: 400\n",
      "Epoch: 82 / 300 Step: 500\n",
      "Epoch: 82 / 300 Step: 600\n",
      "Epoch 82 | train loss:0.2014 | test accuracy:0.3008\n",
      "Epoch: 83 / 300 Step: 0\n",
      "Epoch: 83 / 300 Step: 100\n",
      "Epoch: 83 / 300 Step: 200\n",
      "Epoch: 83 / 300 Step: 300\n",
      "Epoch: 83 / 300 Step: 400\n",
      "Epoch: 83 / 300 Step: 500\n",
      "Epoch: 83 / 300 Step: 600\n",
      "Epoch 83 | train loss:0.1933 | test accuracy:0.3571\n",
      "Epoch: 84 / 300 Step: 0\n",
      "Epoch: 84 / 300 Step: 100\n",
      "Epoch: 84 / 300 Step: 200\n",
      "Epoch: 84 / 300 Step: 300\n",
      "Epoch: 84 / 300 Step: 400\n",
      "Epoch: 84 / 300 Step: 500\n",
      "Epoch: 84 / 300 Step: 600\n",
      "Epoch 84 | train loss:0.1713 | test accuracy:0.2881\n",
      "Epoch: 85 / 300 Step: 0\n",
      "Epoch: 85 / 300 Step: 100\n",
      "Epoch: 85 / 300 Step: 200\n",
      "Epoch: 85 / 300 Step: 300\n",
      "Epoch: 85 / 300 Step: 400\n",
      "Epoch: 85 / 300 Step: 500\n",
      "Epoch: 85 / 300 Step: 600\n",
      "Epoch 85 | train loss:0.3120 | test accuracy:0.3266\n",
      "Epoch: 86 / 300 Step: 0\n",
      "Epoch: 86 / 300 Step: 100\n",
      "Epoch: 86 / 300 Step: 200\n",
      "Epoch: 86 / 300 Step: 300\n",
      "Epoch: 86 / 300 Step: 400\n",
      "Epoch: 86 / 300 Step: 500\n",
      "Epoch: 86 / 300 Step: 600\n",
      "Epoch 86 | train loss:0.1661 | test accuracy:0.3287\n",
      "Epoch: 87 / 300 Step: 0\n",
      "Epoch: 87 / 300 Step: 100\n",
      "Epoch: 87 / 300 Step: 200\n",
      "Epoch: 87 / 300 Step: 300\n",
      "Epoch: 87 / 300 Step: 400\n",
      "Epoch: 87 / 300 Step: 500\n",
      "Epoch: 87 / 300 Step: 600\n",
      "Epoch 87 | train loss:0.3351 | test accuracy:0.2864\n",
      "Epoch: 88 / 300 Step: 0\n",
      "Epoch: 88 / 300 Step: 100\n",
      "Epoch: 88 / 300 Step: 200\n",
      "Epoch: 88 / 300 Step: 300\n",
      "Epoch: 88 / 300 Step: 400\n",
      "Epoch: 88 / 300 Step: 500\n",
      "Epoch: 88 / 300 Step: 600\n",
      "Epoch 88 | train loss:0.3362 | test accuracy:0.2884\n",
      "Epoch: 89 / 300 Step: 0\n",
      "Epoch: 89 / 300 Step: 100\n",
      "Epoch: 89 / 300 Step: 200\n",
      "Epoch: 89 / 300 Step: 300\n",
      "Epoch: 89 / 300 Step: 400\n",
      "Epoch: 89 / 300 Step: 500\n",
      "Epoch: 89 / 300 Step: 600\n",
      "Epoch 89 | train loss:0.1270 | test accuracy:0.3099\n",
      "Epoch: 90 / 300 Step: 0\n",
      "Epoch: 90 / 300 Step: 100\n",
      "Epoch: 90 / 300 Step: 200\n",
      "Epoch: 90 / 300 Step: 300\n",
      "Epoch: 90 / 300 Step: 400\n",
      "Epoch: 90 / 300 Step: 500\n",
      "Epoch: 90 / 300 Step: 600\n",
      "Epoch 90 | train loss:0.1994 | test accuracy:0.2880\n",
      "Epoch: 91 / 300 Step: 0\n",
      "Epoch: 91 / 300 Step: 100\n",
      "Epoch: 91 / 300 Step: 200\n",
      "Epoch: 91 / 300 Step: 300\n",
      "Epoch: 91 / 300 Step: 400\n",
      "Epoch: 91 / 300 Step: 500\n",
      "Epoch: 91 / 300 Step: 600\n",
      "Epoch 91 | train loss:0.2272 | test accuracy:0.3054\n",
      "Epoch: 92 / 300 Step: 0\n",
      "Epoch: 92 / 300 Step: 100\n",
      "Epoch: 92 / 300 Step: 200\n",
      "Epoch: 92 / 300 Step: 300\n",
      "Epoch: 92 / 300 Step: 400\n",
      "Epoch: 92 / 300 Step: 500\n",
      "Epoch: 92 / 300 Step: 600\n",
      "Epoch 92 | train loss:0.3426 | test accuracy:0.2908\n",
      "Epoch: 93 / 300 Step: 0\n",
      "Epoch: 93 / 300 Step: 100\n",
      "Epoch: 93 / 300 Step: 200\n",
      "Epoch: 93 / 300 Step: 300\n",
      "Epoch: 93 / 300 Step: 400\n",
      "Epoch: 93 / 300 Step: 500\n",
      "Epoch: 93 / 300 Step: 600\n",
      "Epoch 93 | train loss:0.4018 | test accuracy:0.2867\n",
      "Epoch: 94 / 300 Step: 0\n",
      "Epoch: 94 / 300 Step: 100\n",
      "Epoch: 94 / 300 Step: 200\n",
      "Epoch: 94 / 300 Step: 300\n",
      "Epoch: 94 / 300 Step: 400\n",
      "Epoch: 94 / 300 Step: 500\n",
      "Epoch: 94 / 300 Step: 600\n",
      "Epoch 94 | train loss:0.2203 | test accuracy:0.2822\n",
      "Epoch: 95 / 300 Step: 0\n",
      "Epoch: 95 / 300 Step: 100\n",
      "Epoch: 95 / 300 Step: 200\n",
      "Epoch: 95 / 300 Step: 300\n",
      "Epoch: 95 / 300 Step: 400\n",
      "Epoch: 95 / 300 Step: 500\n",
      "Epoch: 95 / 300 Step: 600\n",
      "Epoch 95 | train loss:0.5460 | test accuracy:0.3191\n",
      "Epoch: 96 / 300 Step: 0\n",
      "Epoch: 96 / 300 Step: 100\n",
      "Epoch: 96 / 300 Step: 200\n",
      "Epoch: 96 / 300 Step: 300\n",
      "Epoch: 96 / 300 Step: 400\n",
      "Epoch: 96 / 300 Step: 500\n",
      "Epoch: 96 / 300 Step: 600\n",
      "Epoch 96 | train loss:0.4079 | test accuracy:0.3103\n",
      "Epoch: 97 / 300 Step: 0\n",
      "Epoch: 97 / 300 Step: 100\n",
      "Epoch: 97 / 300 Step: 200\n",
      "Epoch: 97 / 300 Step: 300\n",
      "Epoch: 97 / 300 Step: 400\n",
      "Epoch: 97 / 300 Step: 500\n",
      "Epoch: 97 / 300 Step: 600\n",
      "Epoch 97 | train loss:0.2229 | test accuracy:0.2738\n",
      "Epoch: 98 / 300 Step: 0\n",
      "Epoch: 98 / 300 Step: 100\n",
      "Epoch: 98 / 300 Step: 200\n",
      "Epoch: 98 / 300 Step: 300\n",
      "Epoch: 98 / 300 Step: 400\n",
      "Epoch: 98 / 300 Step: 500\n",
      "Epoch: 98 / 300 Step: 600\n",
      "Epoch 98 | train loss:0.1863 | test accuracy:0.2836\n",
      "Epoch: 99 / 300 Step: 0\n",
      "Epoch: 99 / 300 Step: 100\n",
      "Epoch: 99 / 300 Step: 200\n",
      "Epoch: 99 / 300 Step: 300\n",
      "Epoch: 99 / 300 Step: 400\n",
      "Epoch: 99 / 300 Step: 500\n",
      "Epoch: 99 / 300 Step: 600\n",
      "Epoch 99 | train loss:0.4602 | test accuracy:0.2825\n",
      "Epoch: 100 / 300 Step: 0\n",
      "Epoch: 100 / 300 Step: 100\n",
      "Epoch: 100 / 300 Step: 200\n",
      "Epoch: 100 / 300 Step: 300\n",
      "Epoch: 100 / 300 Step: 400\n",
      "Epoch: 100 / 300 Step: 500\n",
      "Epoch: 100 / 300 Step: 600\n",
      "Epoch 100 | train loss:0.1732 | test accuracy:0.2942\n",
      "Epoch: 101 / 300 Step: 0\n",
      "Epoch: 101 / 300 Step: 100\n",
      "Epoch: 101 / 300 Step: 200\n",
      "Epoch: 101 / 300 Step: 300\n",
      "Epoch: 101 / 300 Step: 400\n",
      "Epoch: 101 / 300 Step: 500\n",
      "Epoch: 101 / 300 Step: 600\n",
      "Epoch 101 | train loss:0.3381 | test accuracy:0.2887\n",
      "Epoch: 102 / 300 Step: 0\n",
      "Epoch: 102 / 300 Step: 100\n",
      "Epoch: 102 / 300 Step: 200\n",
      "Epoch: 102 / 300 Step: 300\n",
      "Epoch: 102 / 300 Step: 400\n",
      "Epoch: 102 / 300 Step: 500\n",
      "Epoch: 102 / 300 Step: 600\n",
      "Epoch 102 | train loss:0.1728 | test accuracy:0.2915\n",
      "Epoch: 103 / 300 Step: 0\n",
      "Epoch: 103 / 300 Step: 100\n",
      "Epoch: 103 / 300 Step: 200\n",
      "Epoch: 103 / 300 Step: 300\n",
      "Epoch: 103 / 300 Step: 400\n",
      "Epoch: 103 / 300 Step: 500\n",
      "Epoch: 103 / 300 Step: 600\n",
      "Epoch 103 | train loss:0.1794 | test accuracy:0.2796\n",
      "Epoch: 104 / 300 Step: 0\n",
      "Epoch: 104 / 300 Step: 100\n",
      "Epoch: 104 / 300 Step: 200\n",
      "Epoch: 104 / 300 Step: 300\n",
      "Epoch: 104 / 300 Step: 400\n",
      "Epoch: 104 / 300 Step: 500\n",
      "Epoch: 104 / 300 Step: 600\n",
      "Epoch 104 | train loss:0.1887 | test accuracy:0.2832\n",
      "Epoch: 105 / 300 Step: 0\n",
      "Epoch: 105 / 300 Step: 100\n",
      "Epoch: 105 / 300 Step: 200\n",
      "Epoch: 105 / 300 Step: 300\n",
      "Epoch: 105 / 300 Step: 400\n",
      "Epoch: 105 / 300 Step: 500\n",
      "Epoch: 105 / 300 Step: 600\n",
      "Epoch 105 | train loss:0.2077 | test accuracy:0.3054\n",
      "Epoch: 106 / 300 Step: 0\n",
      "Epoch: 106 / 300 Step: 100\n",
      "Epoch: 106 / 300 Step: 200\n",
      "Epoch: 106 / 300 Step: 300\n",
      "Epoch: 106 / 300 Step: 400\n",
      "Epoch: 106 / 300 Step: 500\n",
      "Epoch: 106 / 300 Step: 600\n",
      "Epoch 106 | train loss:0.1722 | test accuracy:0.2889\n",
      "Epoch: 107 / 300 Step: 0\n",
      "Epoch: 107 / 300 Step: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107 / 300 Step: 200\n",
      "Epoch: 107 / 300 Step: 300\n",
      "Epoch: 107 / 300 Step: 400\n",
      "Epoch: 107 / 300 Step: 500\n",
      "Epoch: 107 / 300 Step: 600\n",
      "Early Stop in Epoch 107 | train loss:0.2203 | test accuracy:0.2740\n",
      "Best Epoch 97 | test accuracy:0.2738\n"
     ]
    }
   ],
   "source": [
    "td = tdata\n",
    "tl = tlabel\n",
    "ttd = sdata\n",
    "ttl = slabel\n",
    "tj = tajmtx\n",
    "gcn = GCN(2048, 16, 1)\n",
    "gcn = train_model(gcn,td,tl,tj,lr=0.01, batch_size=32, epoch=300)\n",
    "save_model(gcn, \"gcn_4.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b710a037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "误差： 1.8956462\n",
      "全误差： -37512.883\n",
      "总值： 48418.805\n",
      "误差： 0.27378988\n",
      "全误差： -4702.0825\n",
      "总值： 45462.54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27378988"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(gcn,ttd,ttl,sajmtx)\n",
    "test_model(gcn,td,tl,tajmtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a11aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
